import time
from eth_bloom import BloomFilter
from web3 import Web3
import winsound
import random
import matplotlib.pyplot as plt
import numpy as np


# This funtion is used for the creation of the Scrap Blockchain Database
def Scrap_Blockchain_Data(Starting_block: int, Ending_block: int) -> None:
    # This function scrapes data from the Ethereum blockchain and stores them locally.
    # Two files are created in computer memory in the specified path.
    # The file that is named "Logs for blocks x to y" contains all the logs that are located in the blocks x to y.
    # The file that is named "Bloom filters for blocks x to y" contains the bloom filters of the blocks x to y.

    web3 = Web3(Web3.HTTPProvider('https://mainnet.infura.io/v3/b2fe92b1dfff4b16a651eb5353bc0dd2'))

    # path: local path
    path = "C:/Users/user/PycharmProjects/Bloom Filters/Scrap Blockchain Database/"


    # File_BloomFilters: file in which the bloom filters are stored
    # File_Logs: file in which the logs are stored
    File_BloomFilters = open(path + "Bloom filters for blocks " + str(Starting_block)+ " to " + str(Ending_block), "w")
    File_Logs = open(path + "Logs for blocks " + str(Starting_block) + " to " + str(Ending_block), "w")

    get_block_error_counter = 0
    event_filter_error_counter = 0
    get_all_entries_error_counter = 0

    for count, block_number in enumerate(range(Starting_block, Ending_block)):
        ## Storing the bloom filter ##
        ## The bloom filter is stored in the form of an integer. Each bloom filter is separated from the next with \n.

        while True:
            try:
                block = web3.eth.get_block(block_number)
                break
            except:
                print("get_block error in block:",block_number)
                winsound.PlaySound("SystemAsterisk", winsound.SND_ALIAS)
                get_block_error_counter = get_block_error_counter + 1


        # bloom_filter: integer form of the bloom filter
        bloom_filter = int(block["logsBloom"].hex(), 0)
        File_BloomFilters.write(str(bloom_filter) + "\n")

        ## Storing the logs ##

        ## The logs are stored in the following form log = "address topic1 topic2 topic3 topic4 transactionHash"
        ## The number of topics is 0,1,2,3 or 4 so topic1, tipic2, topic3 and topic4 may not exist.
        ## The address, topics and transactionHash are in hexadecimal form.
        ## The logs of the same block are separated by ",".
        ## The last log of a block is separated from the first log of the next block by ",\n"

        ## Example ##
        ## The database that is created looks like this:
        ## 0xC02 0xddf 0xff8 0x3c9 0x11b,0x2b8 0x3ef 0x3c9 0xff8 0x11b, ...
        ## Where 0xC02 is the address, 0xddf 0xff8 0x3c9 are the topics, 0x11b is the transactionHash of the first log

        while True:
            try:
                event_filter = web3.eth.filter({
                    "fromBlock": block_number,
                    "toBlock": block_number,
                    # 'topics':[event_signature_hash, "0x000000000000000000000000000000000000000000000000000000000000000a"],
                })
                break
            except:
                print("event_filter error in block:",block_number)
                winsound.PlaySound("SystemAsterisk", winsound.SND_ALIAS)
                event_filter_error_counter = event_filter_error_counter + 1

        # Entries: list of all the logs of the block

        while True:
            try:
                Entries = event_filter.get_all_entries()
                break
            except:
                print("get_all_entries error in block:",block_number)
                winsound.PlaySound("SystemAsterisk", winsound.SND_ALIAS)
                get_all_entries_error_counter = get_all_entries_error_counter + 1



        for entry in Entries:
            File_Logs.write(str(entry["address"]) + " ")
            topic_list = entry["topics"]
            for topic in topic_list:
                File_Logs.write(str(topic.hex()) + " ")
            File_Logs.write(str(entry["transactionHash"].hex()))
            File_Logs.write(",")
        File_Logs.write("\n")

        print(count, " blocks have been created")

    print("get_block errors occurred: ", get_block_error_counter)
    print("event_filter errors occurred: ", event_filter_error_counter)
    print("get_all_entries errors occurred: ", get_all_entries_error_counter)

    File_BloomFilters.close()
    File_Logs.close()



    # Transactions = block["transactions"]
    # for transaction in Transactions:
    #     Receipt = web3.eth.get_transaction_receipt(transaction)
    #     logs = Receipt["logs"]
    #     for log in logs:
    #         File_Logs.write(str(log["address"]) + " ")
    #         topic_list = log["topics"]
    #         for topic in topic_list:
    #             File_Logs.write(str(topic.hex()) + " ")
    #         File_Logs.write(",")
    # File_Logs.write("\n")
    # print(count, " blocks have been created")


# This function is used to search a log utilizing the bloom filters.
def Log_Retrieval_bloom_filter(address: str, topics: list, Starting_block: int, Ending_block: int,  transactionHash:str="none"):
    # This function needs the address and topics of the log to query and optionally the transactionHash.
    # It is also necessary to specify the blocks to query with the Starting_block and Ending_block parameters.
    # The return of the function are two lists. The first one for the blocks in which the log was found and the second
    # one for the corresponding log indexes in each block.
    # If the log is not found, the function returns two empty lists

    path = "C:/Users/user/PycharmProjects/Bloom Filters/Scrap Blockchain Database/"

    File_Logs = open(path + "Logs for blocks " + str(Starting_block) + " to " + str(Ending_block), "r")
    File_BloomFilters = open(path + "Bloom filters for blocks " + str(Starting_block)+ " to "+ str(Ending_block), "r")

    blocks_of_found_element = []
    positions_of_found_element_in_block = []

    # All_logs is the variable containing all the logs of all the blocks
    All_logs = File_Logs.readlines()

    for count, block_number in enumerate(range(Starting_block, Ending_block)):
        # The block_logs list contains all the logs of one block
        block_logs = All_logs[count].split(",")

        bloom_filter = BloomFilter(int(File_BloomFilters.readline()))

        topic_flag = 1
        for topic in topics:
            if bytes.fromhex(topic[2:]) not in bloom_filter:
                topic_flag = 0

        if bytes.fromhex(address[2:]) in bloom_filter:
            if topic_flag:
                for count2, log in enumerate(block_logs):
                    log = log.split(" ")      ##  log = [ address, topic1, topic2, topic3, topic4, transactionHash]
                    if address == log[0]:     ##  the number of topics is 0,1,2,3 or 4
                        if topics == log[1:-1]:
                            if transactionHash == "none":
                                blocks_of_found_element.append(block_number)
                                positions_of_found_element_in_block.append(count2)
                                # results.append((block_number,count2))
                                # break
                            elif transactionHash == log[-1]:
                                blocks_of_found_element.append(block_number)
                                positions_of_found_element_in_block.append(count2)
                                # results.append((block_number,count2))
                                # break
        if (transactionHash != "none") & (blocks_of_found_element != []): break

    return blocks_of_found_element, positions_of_found_element_in_block # , results



# This function is used to search a log without utilizing the bloom filters.
def Log_Retrieval_brute_force(address: str, topics: list, Starting_block: int, Ending_block: int, transactionHash:str="none"):
    # This function needs the address and topics of the log to query and optionally the transactionHash.
    # It is also necessary to specify the blocks to query with the Starting_block and Ending_block parameters.
    # The return of the function are two lists. The first one for the blocks in which the log was found and the second
    # one for the corresponding log indexes in each block.
    # If the log is not found, the function returns two empty lists

    path = "C:/Users/user/PycharmProjects/Bloom Filters/Scrap Blockchain Database/"
    File_Logs = open(path + "Logs for blocks " + str(Starting_block) + " to " + str(Ending_block), "r")

    blocks_of_found_element = []
    positions_of_found_element_in_block = []

    # All_logs is the variable containing all the logs of all the blocks
    All_logs = File_Logs.readlines()

    # every log of every block is checked
    for count,block_number in enumerate(range(Starting_block, Ending_block)):
        # block_logs is the logs that are contained in the block
        block_logs = All_logs[count].split(",")

        for count, log in enumerate(block_logs):
            # log is a list of the following form log = [address, topic1, topic2, topic3, topic4, transactionHash]
            log = log.split(" ")

            # Checking if the log is identical with the one that was specified in the function parameters
            if address == log[0]:
                if topics == log[1:-1]:
                    if transactionHash == "none":
                        blocks_of_found_element.append(block_number)
                        positions_of_found_element_in_block.append(count)
                    elif transactionHash == log[-1]:
                        blocks_of_found_element.append(block_number)
                        positions_of_found_element_in_block.append(count)
        # if the transactionHash is specified the search is stopped when one log is found otherwise the search continues
        if (transactionHash != "none") & (blocks_of_found_element != []): break

    return blocks_of_found_element, positions_of_found_element_in_block



# This function tests the log retrieval time based on the log topic number
def Log_Retrieval_TimeComparison_TopicBased_Test_plot(Number_of_1_2_3_topic_logs: tuple, Scrap_database: tuple, Non_existing_log_database: tuple):
    # The arguments that are required are the number of 1 log topics, number of 2 log topics, number of 3 log topics,
    # this is given in the form of a tuple with 3 positions where 1st position is the number of 1 log topics,
    # 2nd position is the number of 2 log topics and 3rd position is the number of 3 log topics.
    #
    # The second argument is the Scrap database. This is given in the form of a tuple where the 1st position is the
    # starting block of the database and the 2nd position is the ending block of the database
    #
    # The third argument is the non existing log database. This is the database from which the non existing logs are
    # selected to be queried in the Scrap database. This is given in the form of a tuple where the 1st position is the
    # starting block of the database and the 2nd position is the ending block of the database
    #
    # The databases is the stored locally in the computer memory and it is retrieved from the database when
    # the Scrap_database variable is given

    number_of_1_topic_logs, number_of_2_topic_logs, number_of_3_topic_logs = Number_of_1_2_3_topic_logs
    Starting_block,Ending_block = Scrap_database
    non_existing_starting_block, non_existing_ending_block = Non_existing_log_database

    path = "C:/Users/user/PycharmProjects/Bloom Filters/Scrap Blockchain Database/"
    File_Logs = open(path + "Logs for blocks " + str(Starting_block) + " to " + str(Ending_block), "r")

    All_logs = File_Logs.readlines()

    File_Logs.close()

    ##  ##  Data Selection  ##  ##
    ##  Existing log selection  ##

    # one_topic_logs,two_topic_logs, three_topic_logs are the 1,2,3 topic logs that are randomly selected to be queried
    one_topic_logs = []
    two_topic_logs = []
    three_topic_logs = []

    random.seed(0)

    flag = 1
    # The while loop is terminated only when the flag is set to 0
    while flag == 1:
        # random_block_number: a random number between the number 0 and the number of blocks in the database
        random_block_number = random.randint(0,Ending_block - Starting_block - 1)

        # The Entries list is a list containing all the logs of the specific block
        Entries = All_logs[random_block_number].split(",")

        # entry is the random entry that is selected.
        # entry is initially a string but becomes a list representing the log in the following manner:
        # entry = [address, topic1, topic2, topic3, topic4, transactionHash]
        entry = random.choice(Entries) # entry: string
        entry = entry.split(" ") # entry: list

        # num_of_topics is the number of topics
        num_of_topics = len(entry[1:-1])

        # if the number of topics is 1 and the one_topic_logs list is not full, then the entry is appended in the
        # one_topic_logs.
        # if the number of topics is 2 and the two_topic_logs list is not full, then the entry is appended in the
        # two_topic_logs.
        # if the number of topics is 3 and the three_topic_logs list is not full, then the entry is appended in the
        # three_topic_logs.
        if (num_of_topics == 1) & (len(one_topic_logs) < number_of_1_topic_logs):
            one_topic_logs.append(entry)
        elif (num_of_topics == 2) & (len(two_topic_logs) < number_of_2_topic_logs):
            two_topic_logs.append(entry)
        elif (num_of_topics == 3) & (len(three_topic_logs) < number_of_3_topic_logs):
            three_topic_logs.append(entry)

        # The while loop stops only when the one_topic_logs, two_topic_logs, three_topic_logs are full
        if (len(one_topic_logs) == number_of_1_topic_logs) & (len(two_topic_logs) == number_of_2_topic_logs) & (len(three_topic_logs) == number_of_3_topic_logs):
            flag = 0

    ##  Non existing log selection  ##

    File_non_existing_Logs = open(path + "Logs for blocks " + str(non_existing_starting_block) + " to " + str(non_existing_ending_block), "r")
    All_non_existing_logs = File_non_existing_Logs.readlines()
    File_non_existing_Logs.close()

    # non_exist_one_topic_logs,non_exist_two_topic_logs, non_exist_three_topic_logs are the non existing 1,2,3 topic
    # logs that are randomly selected to be queried
    non_exist_one_topic_logs = []
    non_exist_two_topic_logs = []
    non_exist_three_topic_logs = []

    num_of_non_exist_1_topic_logs = 0
    num_of_non_exist_2_topic_logs = 0
    num_of_non_exist_3_topic_logs = 0

    flag = 1
    # The while loop is terminated only when the flag is set to 0
    while flag == 1:
        # random_block_number: a random number between the number 0 and the number of blocks in the database
        random_block_number = random.randint(0, non_existing_ending_block - non_existing_starting_block - 1)

        # The Entries list is a list containing all the logs of the specific block
        Entries = All_non_existing_logs[random_block_number].split(",")

        # entry is the random entry that is selected.
        # entry is initially a string but becomes a list representing the log in the following manner:
        # entry = [address, topic1, topic2, topic3, topic4, transactionHash]
        entry = random.choice(Entries) # entry: string
        entry = entry.split(" ") # entry: list

        # num_of_topics is the number of topics
        num_of_topics = len(entry[1:-1])

        # if the number of topics is 1 and the non_exist_one_topic_logs list is not full, then the entry is
        # appended in the one_topic_logs.
        # if the number of topics is 2 and the non_exist_two_topic_logs list is not full, then the entry is
        # appended in the two_topic_logs.
        # if the number of topics is 3 and the non_exist_three_topic_logs list is not full, then the entry is
        # appended in the three_topic_logs.
        if (num_of_topics == 1) & (len(non_exist_one_topic_logs) < number_of_1_topic_logs):
            non_exist_one_topic_logs.append(entry)

        elif (num_of_topics == 2) & (len(non_exist_two_topic_logs) < number_of_2_topic_logs):
            non_exist_two_topic_logs.append(entry)

        elif (num_of_topics == 3) & (len(non_exist_three_topic_logs) < number_of_3_topic_logs):
            non_exist_three_topic_logs.append(entry)

        # The while loop stops only when the non_exist_one_topic_logs, non_exist_two_topic_logs,
        # non_exist_three_topic_logs are full
        if (len(non_exist_one_topic_logs) == number_of_1_topic_logs) & (len(non_exist_two_topic_logs) == number_of_2_topic_logs) & (
                len(non_exist_three_topic_logs) == number_of_3_topic_logs):
            flag = 0


    ##  ##  Retrieval time recording  ##  ##

    ##  existing logs bloom filter retrieval time  ##
    # The bloom filter retrieval time for all the logs in the one_topic_logs list is time_bloom2 - time_bloom1
    time_bloom1 = time.time()
    for log in one_topic_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
    time_bloom2 = time.time()

    # The bloom filter retrieval time for all the logs in the two_topic_logs list is time_bloom3 - time_bloom2
    for log in two_topic_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
    time_bloom3 = time.time()

    # The bloom filter retrieval time for all the logs in the three_topic_logs list is time_bloom4 - time_bloom3
    for log in three_topic_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
    time_bloom4 = time.time()

    ##  Non existing logs bloom filter retrieval time  ##
    # The bloom filter retrieval time for all the logs in the non_exist_one_topic_logs list is time_bloom5 - time_bloom4
    for log in non_exist_one_topic_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
        if block != []: print("Found in ","one_topic_logs","bloom")
    time_bloom5 = time.time()

    # The bloom filter retrieval time for all the logs in the non_exist_two_topic_logs list is time_bloom6 - time_bloom5
    for log in non_exist_two_topic_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
        if block != []: print("Found in ", "two_topic_logs","bloom")
    time_bloom6 = time.time()

    # The bloom filter retrieval time for all the logs in the non_exist_three_topic_logs list is time_bloom7 - time_bloom6
    for log in non_exist_three_topic_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
        if block != []: print("Found in ","three_topic_logs","bloom")
    time_bloom7 = time.time()

    # The one_topic_bloom_time, two_topic_bloom_time, three_topic_bloom_time are the mean bloom filter retrieval times
    # for the one, two and three topic logs that were selected.
    # The total_topic_bloom_time is the mean bloom filter retrieval time for all the logs regardless of the number of
    # topics.
    one_topic_bloom_time = (time_bloom2 - time_bloom1)/number_of_1_topic_logs
    two_topic_bloom_time = (time_bloom3 - time_bloom2)/number_of_2_topic_logs
    three_topic_bloom_time = (time_bloom4 - time_bloom3)/number_of_3_topic_logs
    total_topic_bloom_time = (time_bloom4 - time_bloom1)/(number_of_1_topic_logs+number_of_2_topic_logs+ number_of_3_topic_logs)

    # The non_exist_one_topic_bloom_time, non_exist_two_topic_bloom_time, non_exist_three_topic_bloom_time are the
    # mean bloom filter retrieval times for the one, two and three topic non existing logs that were selected.
    # The non_exist_total_topic_bloom_time is the mean bloom filter retrieval time for all the logs regardless of
    # the number of topics.
    non_exist_one_topic_bloom_time = (time_bloom5 - time_bloom4)/number_of_1_topic_logs
    non_exist_two_topic_bloom_time = (time_bloom6 - time_bloom5)/number_of_2_topic_logs
    non_exist_three_topic_bloom_time = (time_bloom7 - time_bloom6)/number_of_3_topic_logs
    non_exist_total_topic_bloom_time = (time_bloom7 - time_bloom4)/(number_of_1_topic_logs+number_of_2_topic_logs+ number_of_3_topic_logs)


    ## Brute force retrieval time ##
    # The brute force retrieval time for all the logs in the one_topic_logs list is time_brute2 - time_brute1
    time_brute1 = time.time()
    for log in one_topic_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
    time_brute2 = time.time()

    # The brute force retrieval time for all the logs in the two_topic_logs list is time_brute3 - time_brute2
    for log in two_topic_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
    time_brute3 = time.time()

    # The brute force retrieval time for all the logs in the three_topic_logs list is time_brute4 - time_brute3
    for log in three_topic_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
    time_brute4 = time.time()

    ##  Non existing logs brute force retrieval time  ##
    # The brute force retrieval time for all the logs in the non_exist_one_topic_logs list is time_brute5 - time_brute4
    for log in non_exist_one_topic_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
        if block != []: print("Found in ","one_topic_logs","brute")
    time_brute5 = time.time()

    # The brute force retrieval time for all the logs in the non_exist_two_topic_logs list is time_brute6 - time_brute5
    for log in non_exist_two_topic_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
        if block != []: print("Found in ","two_topic_logs","brute")
    time_brute6 = time.time()

    # The brute force retrieval time for all the logs in the non_exist_three_topic_logs list is time_brute7 - time_brute6
    for log in non_exist_three_topic_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
        if block != []: print("Found in ","three_topic_logs","brute")
    time_brute7 = time.time()


    # The one_topic_brute_time, two_topic_brute_time, three_topic_brute_time are the mean brute force retrieval times
    # for the one, two and three topic logs that were selected.
    # The total_topic_brute_time is the mean brute force retrieval time for all the logs regardless of the number of
    # topics.
    one_topic_brute_time = (time_brute2 - time_brute1)/number_of_1_topic_logs
    two_topic_brute_time = (time_brute3 - time_brute2)/number_of_2_topic_logs
    three_topic_brute_time = (time_brute4 - time_brute3)/number_of_3_topic_logs
    total_topic_brute_time = (time_brute4 - time_brute1)/(number_of_1_topic_logs+number_of_2_topic_logs+ number_of_3_topic_logs)

    # The non_exist_one_topic_brute_time, non_exist_two_topic_brute_time, non_exist_three_topic_brute_time are the
    # mean brute force retrieval times for the one, two and three topic non existing logs that were selected.
    # The non_exist_total_topic_brute_time is the mean brute force retrieval time for all the logs regardless of
    # the number of topics.
    non_exist_one_topic_brute_time = (time_brute5 - time_brute4)/number_of_1_topic_logs
    non_exist_two_topic_brute_time = (time_brute6 - time_brute5)/number_of_2_topic_logs
    non_exist_three_topic_brute_time = (time_brute7 - time_brute6)/number_of_3_topic_logs
    non_exist_total_topic_brute_time = (time_brute7 - time_brute4)/(number_of_1_topic_logs+number_of_2_topic_logs+ number_of_3_topic_logs)

    ##  ##  Plot  ##  ##
    x = np.arange(4)
    bloom_measurments = [one_topic_bloom_time, two_topic_bloom_time, three_topic_bloom_time, total_topic_bloom_time]
    brute_measurments = [one_topic_brute_time, two_topic_brute_time, three_topic_brute_time, total_topic_brute_time]
    non_exist_bloom_measurments = [non_exist_one_topic_bloom_time, non_exist_two_topic_bloom_time,non_exist_three_topic_bloom_time, non_exist_total_topic_bloom_time]
    non_exist_brute_measurments = [non_exist_one_topic_brute_time, non_exist_two_topic_brute_time,non_exist_three_topic_brute_time, non_exist_total_topic_brute_time]
    width = 0.10

    # plot data in grouped manner of bar type
    plt.bar(x - 0.15, bloom_measurments, width, color='lightseagreen')
    plt.bar(x - 0.05, brute_measurments, width, color='navy')
    plt.bar(x + 0.05, non_exist_bloom_measurments, width, color='indianred')
    plt.bar(x + 0.15, non_exist_brute_measurments, width, color='darkred')

    plt.xticks(x, ['One topic logs\n('+str(number_of_1_topic_logs)+")", 'Two topic logs\n('+str(number_of_2_topic_logs)+")", 'Three topic logs\n('+str(number_of_3_topic_logs)+")", 'Total\n('+str(number_of_1_topic_logs+number_of_2_topic_logs+number_of_3_topic_logs)+")"])
    # plt.xlabel("Teams")
    plt.ylabel("Mean time")
    plt.legend(["Bloom filter", "Brute force","Bloom filter (non existing logs)", "Brute force (non existing logs)" ], bbox_to_anchor=(1, -1), loc='lower center')
    plt.title('Performance Comparison')
    plt.show()


# This function tests the log retrieval time based on the log position
def Log_Retrieval_TimeComparison_PositionBased_Test_plot(Number_of_1_2_3_topic_logs: tuple, Scrap_database: tuple):
    # The arguments that are required are the number of 1 log topics, number of 2 log topics, number of 3 log topics,
    # this is given in the form of a tuple with 3 positions where 1st position is the number of 1 log topics,
    # 2nd position is the number of 2 log topics and 3rd position is the number of 3 log topics.
    # The second argument is the Scrap database. This is given in the form of a tuple where the 1st position is the
    # starting block of the database and the 2nd position is the ending block of the database
    # The database is the stored locally in the computer memory and it is retrieved from the database when
    # the Scrap_database variable is given
    number_of_1_topic_logs, number_of_2_topic_logs, number_of_3_topic_logs = Number_of_1_2_3_topic_logs
    Starting_block,Ending_block = Scrap_database

    path = "C:/Users/user/PycharmProjects/Bloom Filters/Scrap Blockchain Database/"
    File_Logs = open(path + "Logs for blocks " + str(Starting_block) + " to " + str(Ending_block), "r")

    All_logs = File_Logs.readlines()


    ##  ##  Data selection  ##  ##

    # start_logs, middle_logs, end_logs are the lists that in the end contains the logs to be tested.
    # In the end, each list must have the specified number of 1 topic, 2 topic and 3 topic logs
    start_logs = []
    middle_logs = []
    end_logs = []

    num_of_1_topic_logs_start = 0
    num_of_2_topic_logs_start = 0
    num_of_3_topic_logs_start = 0

    num_of_1_topic_logs_middle = 0
    num_of_2_topic_logs_middle = 0
    num_of_3_topic_logs_middle = 0

    num_of_1_topic_logs_end = 0
    num_of_2_topic_logs_end = 0
    num_of_3_topic_logs_end = 0

    random.seed(0)

    start_flag = 1
    middle_flag = 1
    end_flag = 1

    number_of_blocks = Ending_block - Starting_block

    # The while loop is terminated only when all three flags are set to 0
    while start_flag + middle_flag + end_flag > 0:
        # random_block_number: a random number between the number 0 and the number of blocks in the database
        random_block_number = random.randint(0, number_of_blocks - 1)

        # if this number is in the first 10% blocks of the database, a random log from this block is selected.
        # Depending from the number of topics of this log, it is either appended into the start_logs, or ignored
        if random_block_number < 0.1 * number_of_blocks:

            # The Entries list is a list containing all the logs of the specific block
            Entries = All_logs[random_block_number].split(",")

            # entry is the random entry that is selected.
            # entry is initially a string but becomes a list representing the log in the following manner:
            # entry = [address, topic1, topic2, topic3, topic4, transactionHash]
            entry = random.choice(Entries) # entry: string
            entry = entry.split(" ") # entry: list

            # num_of_topics is the number of topics
            num_of_topics = len(entry[1:-1])

            # if the start_logs list requires a log with the number of topics of the entry, then the entry is appended
            # in the start_logs list, otherwise it is ignored
            # To keep track of the number of 1, 2 nad 3 topic logs that the start_log list requires, the
            # num_of_1_topic_logs_start, num_of_2_topic_logs_start, num_of_3_topic_logs_start variables are used
            # as counters.
            if (num_of_topics == 1) & (num_of_1_topic_logs_start < number_of_1_topic_logs):
                start_logs.append(entry)
                num_of_1_topic_logs_start = num_of_1_topic_logs_start + 1
            elif (num_of_topics == 2) & (num_of_2_topic_logs_start < number_of_2_topic_logs):
                start_logs.append(entry)
                num_of_2_topic_logs_start = num_of_2_topic_logs_start + 1
            elif (num_of_topics == 3) & (num_of_3_topic_logs_start < number_of_3_topic_logs):
                start_logs.append(entry)
                num_of_3_topic_logs_start = num_of_3_topic_logs_start + 1

            # This flag is set to 0 when all the logs from the start of the database are selected for testing
            if (num_of_1_topic_logs_start == number_of_1_topic_logs) & (num_of_2_topic_logs_start == number_of_2_topic_logs) & (
                    num_of_3_topic_logs_start == number_of_3_topic_logs):
                start_flag = 0


        # if this number is int he 45-55% blocks of the database, a random log from this block is selected.
        # Depending from the number of topics of this log, it is either appended into the middle_logs, or ignored
        elif (random_block_number > 0.45 * number_of_blocks) & (random_block_number < 0.55 * number_of_blocks):
            # The logic is the same as the start_logs

            Entries = All_logs[random_block_number].split(",")

            entry = random.choice(Entries)
            entry = entry.split(" ")

            num_of_topics = len(entry[1:-1])

            if (num_of_topics == 1) & (num_of_1_topic_logs_middle < number_of_1_topic_logs):
                middle_logs.append(entry)
                num_of_1_topic_logs_middle = num_of_1_topic_logs_middle + 1
            elif (num_of_topics == 2) & (num_of_2_topic_logs_middle < number_of_2_topic_logs):
                middle_logs.append(entry)
                num_of_2_topic_logs_middle = num_of_2_topic_logs_middle + 1
            elif (num_of_topics == 3) & (num_of_3_topic_logs_middle < number_of_3_topic_logs):
                middle_logs.append(entry)
                num_of_3_topic_logs_middle = num_of_3_topic_logs_middle + 1

            # This flag is set to 0 when all the logs from the middle of the database are selected for testing
            if (num_of_1_topic_logs_middle == number_of_1_topic_logs) & (
                    num_of_2_topic_logs_middle == number_of_2_topic_logs) & (
                    num_of_3_topic_logs_middle == number_of_3_topic_logs):
                middle_flag = 0

        # if this number is in the last 10% blocks of the database, a random log from this block is selected.
        # Depending from the number of topics of this log, it is either appended into the end_logs, or ignored
        elif random_block_number > 0.9 * number_of_blocks:
            # The logic is the same as the start_logs
            Entries = All_logs[random_block_number].split(",")

            entry = random.choice(Entries)
            entry = entry.split(" ")

            num_of_topics = len(entry[1:-1])

            if (num_of_topics == 1) & (num_of_1_topic_logs_end < number_of_1_topic_logs):
                end_logs.append(entry)
                num_of_1_topic_logs_end = num_of_1_topic_logs_end + 1
            elif (num_of_topics == 2) & (num_of_2_topic_logs_end < number_of_2_topic_logs):
                end_logs.append(entry)
                num_of_2_topic_logs_end = num_of_2_topic_logs_end + 1
            elif (num_of_topics == 3) & (num_of_3_topic_logs_end < number_of_3_topic_logs):
                end_logs.append(entry)
                num_of_3_topic_logs_end = num_of_3_topic_logs_end + 1

            # This flag is set to 0 when all the logs from the middle of the database are selected for testing
            if (num_of_1_topic_logs_end == number_of_1_topic_logs) & (
                    num_of_2_topic_logs_end == number_of_2_topic_logs) & (
                    num_of_3_topic_logs_end == number_of_3_topic_logs):
                end_flag = 0


    ##  ##  Retrieval time recording  ##  ##

    ## Brute force retrieval time ##

    # The bloom filter retrieval time for all the logs in the start_logs list is time_bloom2 - time_bloom1
    time_bloom1 = time.time()
    for log in start_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
    time_bloom2 = time.time()

    # The bloom filter retrieval time for all the logs in the middle_logs list is time_bloom3 - time_bloom2
    for log in middle_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
    time_bloom3 = time.time()

    # The bloom filter retrieval time for all the logs in the end_logs list is time_bloom4 - time_bloom3
    for log in end_logs:
        block, position = Log_Retrieval_bloom_filter(log[0], log[1:-1], Starting_block,Ending_block,transactionHash=log[-1])
    time_bloom4 = time.time()

    # log_number is the total number of logs, that is the same in the start_logs, middle_logs and end_logs lists
    log_number = len(start_logs)

    # The start_bloom_time, middle_bloom_time, end_bloom_time are the mean bloom filter retrieval times for the start,
    # middle and end of the database.
    # The total_bloom_time is the mean bloom filter retrieval time for all the logs regardless of the number of topics.
    start_bloom_time = (time_bloom2 - time_bloom1)/log_number
    middle_bloom_time = (time_bloom3 - time_bloom2)/log_number
    end_bloom_time = (time_bloom4 - time_bloom3)/log_number
    total_bloom_time = (time_bloom4 - time_bloom1) / (log_number * 3)


    ## Brute force retrieval time ##

    # The brute force retrieval time for all the logs in the start_logs list is time_brute2 - time_brute1
    time_brute1 = time.time()
    for log in start_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
    time_brute2 = time.time()

    # The brute force retrieval time for all the logs in the middle_logs list is time_brute2 - time_brute1
    for log in middle_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
    time_brute3 = time.time()

    # The brute force retrieval time for all the logs in the end_logs list is time_brute2 - time_brute1
    for log in end_logs:
        block, position = Log_Retrieval_brute_force(log[0], log[1:-1],
                                                    Starting_block, Ending_block,transactionHash=log[-1])
    time_brute4 = time.time()

    # The start_brute_time, middle_brute_time, end_brute_time are the mean brute force retrieval times for the start,
    # middle and end of the database.
    # The total_brute_time is the mean brute force retrieval time for all the logs regardless of the number of topics.
    start_brute_time = (time_brute2 - time_brute1)/log_number
    middle_brute_time = (time_brute3 - time_brute2)/log_number
    end_brute_time = (time_brute4 - time_brute3)/log_number
    total_brute_time = (time_brute4 - time_brute1) / (log_number * 3)


    ##  ##  Plot  ##  ##
    x = np.arange(4)
    bloom_measurments = [start_bloom_time, middle_bloom_time, end_bloom_time, total_bloom_time]
    brute_measurments = [start_brute_time, middle_brute_time, end_brute_time, total_brute_time]
    width = 0.20

    # plot data in grouped manner of bar type
    plt.bar(x - 0.1, bloom_measurments, width, color='lightseagreen')
    plt.bar(x + 0.1, brute_measurments, width, color='navy')

    plt.xticks(x, ['logs at the start','logs at the middle','logs at the end','Total logs'])

    plt.ylabel("Mean time")
    text = '1 topic logs: '+str(number_of_1_topic_logs)+'\n2 topic logs: '+str(number_of_2_topic_logs) + '\n3 topic logs: '+str(number_of_3_topic_logs)
    plt.text(-0.2, max([max(brute_measurments),max(bloom_measurments)]), text, ha='left', va='top', fontsize=10)
    plt.legend(["Bloom filter", "Brute force"],loc='upper right')
    plt.title('Performance Comparison based on log position')
    plt.show()

